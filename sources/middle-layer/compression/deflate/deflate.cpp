/*******************************************************************************
 * Copyright (C) 2022 Intel Corporation
 *
 * SPDX-License-Identifier: MIT
 ******************************************************************************/

#include "compression/verification/verification_state_builder.hpp"
#include "compression/verification/verify.hpp"
#include "deflate.hpp"
#include "compression/deflate/compression_units/auxiliary_units.hpp"
#include "compression/deflate/implementations/deflate_implementation.hpp"

#include "compression/deflate/streams/hw_deflate_state.hpp"
#include "util/descriptor_processing.hpp"
#include "util/iaa_features_checks.hpp"
#include "util/hw_status_converting.hpp"

#include "dispatcher/hw_dispatcher.hpp"

// core-iaa
#include "hw_aecs_api.h" // HW_AECS_COMPRESS_WITH_HT

namespace qpl::ml::compression {

auto deflate_pass(deflate_state<execution_path_t::software> &stream, uint8_t *begin, uint32_t size) noexcept -> compression_operation_result_t {
    compression_operation_result_t result;

    result.status_code_ = status_list::ok;

    stream.set_source(begin, size);

    compression_state_t state = compression_state_t::init_compression;

    auto implementation = build_implementation<block_type_t::deflate_block>(stream.compression_level(),
                                                                            stream.compression_mode(),
                                                                            stream.mini_blocks_support(),
                                                                            stream.dictionary_support());

    // Main pipeline
    do {
        result.status_code_ = implementation.execute(stream, state);
    } while (!result.status_code_ && state != compression_state_t::finish_compression_process);

    if (!result.status_code_ && stream.mini_blocks_support() == mini_blocks_support_t::disabled) {
        update_checksum(stream);
    }

    return result;
}

template<>
auto deflate<execution_path_t::hardware, deflate_mode_t::deflate_no_headers>(deflate_state<execution_path_t::hardware> &state,
                                                                             uint8_t *begin,
                                                                             const uint32_t size) noexcept -> compression_operation_result_t {
//    auto output_begin_ptr = state.next_out();

    compression_operation_result_t result;

    // Check if dictionary is supported in hardware
    bool is_hw_dict_compress_supported = false;
#if defined( __linux__ )
    static auto &dispatcher = qpl::ml::dispatcher::hw_dispatcher::get_instance();
    if (!dispatcher.is_hw_support()) {
        hw_accelerator_status hw_status = dispatcher.get_hw_init_status();
        result.status_code_ = qpl::ml::util::convert_hw_accelerator_status_to_qpl_status(hw_status);
        return result;
    }

    const auto &device = dispatcher.device(0);
    is_hw_dict_compress_supported = device.get_dict_compress_support();
#endif //__linux__

    // If dictionary is provided, check that the following are true:
    // 1. compression with dictionary is supported
    // 2. this is a single chunk job (multi-chunk dictionary is not supported on HW path).
    // 3. the hardware_dictionary_level is set
    qpl_dictionary *dictionary = state.dictionary_;
    if (dictionary) {
        bool is_single_chunk = (state.is_first_chunk() && state.is_last_chunk());
        if (!is_hw_dict_compress_supported || !is_single_chunk || (hardware_dictionary_level::HW_NONE == dictionary->hw_dict_level)) {
            result.status_code_ = status_list::not_supported_err;
            return result;
        }

        uint32_t dict_size_in_aecs = get_dictionary_size_in_aecs(*dictionary);
        state.meta_data_->aecs_size = HW_AECS_COMPRESS_WITH_HT + dict_size_in_aecs;

    } else {
        state.meta_data_->aecs_size = HW_AECS_COMPRESS_WITH_HT;
    }

    hw_iaa_descriptor_compress_set_termination_rule(state.compress_descriptor_, end_of_block);
    hw_iaa_descriptor_set_input_buffer(state.compress_descriptor_, begin, size);
    hw_iaa_descriptor_compress_set_aecs(state.compress_descriptor_,
                                        state.meta_data_->aecs_,
                                        hw_aecs_access_read,
                                        !qpl::ml::util::are_iaa_gen_2_min_capabilities_present());


    // Setup dictionary in compression descriptor
    if (dictionary) {
        const uint32_t dict_size_in_aecs         = get_dictionary_size_in_aecs(*dictionary);
        const uint8_t *const dictionary_data_ptr = get_dictionary_data(*dictionary);
        uint8_t load_dictionary_val              = get_load_dictionary_flag(*dictionary);

        hw_iaa_descriptor_compress_setup_dictionary(state.compress_descriptor_,
                                                    dict_size_in_aecs,
                                                    dictionary_data_ptr,
                                                    dictionary->aecs_raw_dictionary_offset,
                                                    state.meta_data_->aecs_,
                                                    state.meta_data_->aecs_index,
                                                    state.meta_data_->aecs_size,
                                                    load_dictionary_val);
    }

    result = util::process_descriptor<compression_operation_result_t,
                                           util::execution_mode_t::sync>(state.compress_descriptor_, state.completion_record_);

    if (result.status_code_ == status_list::ok) {
        result.completed_bytes_ = size;
    } else if (result.status_code_ == status_list::destination_is_short_error) {
        // Align with the behavior of non-canned mode deflate overflow (stored block also doesn't fit), which replaces
        // the returned error code "destination_is_short_error" from IntelÂ® In-Memory Analytics Accelerator
        // with "more_output_needed"
        result.status_code_ = status_list::more_output_needed;
    }

//    if (state.verify_descriptor_ && !result.status_code_) {
//        result.indexes_written_ = state.prev_written_indexes;
//
//        hw_iaa_descriptor_compress_verification_write_initial_index(state.verify_descriptor_,
//                                                                    state.aecs_verify_,
//                                                                    0u,
//                                                                    0u);
//        result.indexes_written_ += 1;
//
//        auto header_bit_size = get_deflate_header_bits_size(state.huffman_table_);
//
//        hw_iaa_descriptor_set_input_buffer(state.verify_descriptor_,
//                                           get_deflate_header_ptr(state.huffman_table_),
//                                           util::bit_to_byte(header_bit_size));
//
//        hw_iaa_descriptor_init_inflate_header(state.verify_descriptor_,
//                                              state.aecs_verify_,
//                                              8u - header_bit_size & 7u,
//                                              static_cast<const hw_aecs_access_policy>(hw_aecs_access_write
//                                                                                       | hw_aecs_toggle_rw));
//
//        auto verify_result = util::process_descriptor<verification_pass_result_t,
//                                                      util::execution_mode_t::sync>(state.verify_descriptor_,
//                                                                                    state.completion_record_);
//
//        if (verify_result.status_code_) {
//            result.status_code_ = qpl::ml::status_list::verify_error;
//
//            return result;
//        }
//
//        hw_iaa_descriptor_set_input_buffer(state.verify_descriptor_, output_begin_ptr, result.output_bytes_);
//
//        hw_iaa_descriptor_inflate_set_aecs(state.verify_descriptor_,
//                                           state.aecs_verify_,
//                                           HW_AECS_FILTER_AND_DECOMPRESS,
//                                           hw_aecs_access_read);
//
//        hw_iaa_descriptor_inflate_set_flush(state.verify_descriptor_);
//
//        verify_result = util::process_descriptor<verification_pass_result_t,
//                                                 util::execution_mode_t::sync>(state.verify_descriptor_,
//                                                                               state.completion_record_);
//
//        if (verify_result.status_code_ ||
//            verify_result.checksums_.crc32_ != result.checksums_.crc32_) {
//            result.status_code_ = qpl::ml::status_list::verify_error;
//
//            return result;
//        }
//
//        result.indexes_written_ += verify_result.indexes_written_;
//    }

    return result;
}

template<>
auto deflate<execution_path_t::hardware, deflate_mode_t::deflate_default>(deflate_state<execution_path_t::hardware> &state,
                                                                          uint8_t *source_begin_ptr,
                                                                          const uint32_t source_size) noexcept -> compression_operation_result_t {
    compression_operation_result_t result;
    auto actual_aecs      = state.meta_data_->aecs_index; // AECS used to read
    auto output_begin_ptr = state.next_out();

    // Check if features are supported in hardware
    bool is_hw_header_gen_supported    = false;
    bool is_hw_dict_compress_supported = false;

#if defined( __linux__ )
    static auto &dispatcher = qpl::ml::dispatcher::hw_dispatcher::get_instance();
    if (!dispatcher.is_hw_support()) {
        hw_accelerator_status hw_status = dispatcher.get_hw_init_status();
        result.status_code_ = qpl::ml::util::convert_hw_accelerator_status_to_qpl_status(hw_status);
        return result;
    }

    const auto &device = dispatcher.device(0);
    is_hw_header_gen_supported = device.get_header_gen_support();
    is_hw_dict_compress_supported = device.get_dict_compress_support();
#endif //__linux__

    // If dictionary is provided, check that the following are true:
    // 1. compression with dictionary is supported
    // 2. this is a single chunk job (multi-chunk dictionary is not supported on HW path)
    // 3. the hardware_dictionary_level is set
    qpl_dictionary *dictionary = state.dictionary_;
    if (dictionary) {
        bool is_single_chunk = (state.is_first_chunk() && state.is_last_chunk());
        if (!is_hw_dict_compress_supported || !is_single_chunk || (hardware_dictionary_level::HW_NONE == dictionary->hw_dict_level)) {
            result.status_code_ = status_list::not_supported_err;
            return result;
        }

        uint32_t dict_size_in_aecs = get_dictionary_size_in_aecs(*dictionary);
        state.meta_data_->aecs_size = HW_AECS_COMPRESS_WITH_HT + dict_size_in_aecs;

    } else {
        state.meta_data_->aecs_size = HW_AECS_COMPRESS_WITH_HT;
    }
    hw_iaa_aecs_compress *actual_aecs_ptr = hw_iaa_aecs_compress_get_aecs_ptr(state.meta_data_->aecs_, actual_aecs, state.meta_data_->aecs_size);
    if (!actual_aecs_ptr) {
        result.status_code_ = status_list::internal_error;
        return result;
    }
    state.meta_data_->stored_bits = hw_iaa_aecs_compress_accumulator_get_actual_bits(actual_aecs_ptr);

    // There are two modes for header generation: 1-pass and 2-pass.
    // 1-pass can be used if dictionary is not used and source size is no larger than 4KB.
    // 1-pass header generation will calculate the statistics and Huffman Table together with compression
    // 2-pass header generation will calculate Huffman Table in the 1st pass, and compress data in the 2nd pass
    // (2-pass is similar to dynamic deflate without header generation support)
    bool is_hw_1_pass_header_gen = false;
    bool is_hw_2_pass_header_gen = false;
    if (state.collect_statistic_descriptor_ && is_hw_header_gen_supported) {
        if (source_size <= 4096U && !dictionary) {
            is_hw_1_pass_header_gen = true;
        } else {
            is_hw_2_pass_header_gen = true;
        }
    }

    // Collect statistic
    if (state.collect_statistic_descriptor_) { // Dynamic mode used
        if (is_hw_2_pass_header_gen) {
            hw_iaa_descriptor_init_statistic_collector_with_header_gen(state.collect_statistic_descriptor_,
                                                       source_begin_ptr, source_size,
                                                       state.meta_data_->aecs_, actual_aecs, state.is_last_chunk(), state.is_first_chunk());
            hw_iaa_descriptor_compress_set_mini_block_size(state.collect_statistic_descriptor_,
                                                           state.meta_data_->mini_block_size_);

            /* Append EOB so that EOB will have a Huffman code */
            hw_iaa_descriptor_compress_set_termination_rule(state.collect_statistic_descriptor_,
                                                            hw_iaa_terminator_t::end_of_block);

            // Setup dictionary in statistics collection descriptor
            if (dictionary) {
                const uint32_t dict_size_in_aecs         = get_dictionary_size_in_aecs(*dictionary);
                const uint8_t *const dictionary_data_ptr = get_dictionary_data(*dictionary);
                uint8_t load_dictionary_val              = get_load_dictionary_flag(*dictionary);

                hw_iaa_descriptor_compress_setup_dictionary(state.collect_statistic_descriptor_,
                                                            dict_size_in_aecs,
                                                            dictionary_data_ptr,
                                                            dictionary->aecs_raw_dictionary_offset,
                                                            state.meta_data_->aecs_,
                                                            state.meta_data_->aecs_index,
                                                            state.meta_data_->aecs_size,
                                                            load_dictionary_val);
            }

            // The 1st pass will generate Huffman table and deflate header
            result = util::process_descriptor<compression_operation_result_t,
                                              util::execution_mode_t::sync>(state.collect_statistic_descriptor_,
                                                                            state.completion_record_);

            if (result.status_code_) {
                return result;
            }

            // Invert the AECS toggle for compress because src2 is written as AECS
            state.meta_data_->aecs_index ^= 1U;
            actual_aecs = state.meta_data_->aecs_index;

        } else if (!is_hw_header_gen_supported) {
            hw_iaa_aecs_compress *actual_aecs_ptr = hw_iaa_aecs_compress_get_aecs_ptr(state.meta_data_->aecs_, actual_aecs, state.meta_data_->aecs_size);
            if (!actual_aecs_ptr) {
                result.status_code_ = status_list::internal_error;
                return result;
            }
            // If header generation is not supported, the software will compute the Huffman table
            // based on statistics generated in the 1st pass
            hw_iaa_descriptor_init_statistic_collector(state.collect_statistic_descriptor_,
                                                       source_begin_ptr, source_size,
                                                       &actual_aecs_ptr->histogram);
            hw_iaa_descriptor_compress_set_mini_block_size(state.collect_statistic_descriptor_,
                                                           state.meta_data_->mini_block_size_);

            result = util::process_descriptor<compression_operation_result_t,
                                              util::execution_mode_t::sync>(state.collect_statistic_descriptor_,
                                                                            state.completion_record_);

            if (result.status_code_) {
                return result;
            }

            hw_iaa_aecs_compress_write_deflate_dynamic_header_from_histogram(actual_aecs_ptr,
                                                                             &actual_aecs_ptr->histogram,
                                                                             state.is_last_chunk());
        }

        /* For dynamic mode, append EOB after final token in the compression descriptor */
        hw_iaa_descriptor_compress_set_termination_rule(state.compress_descriptor_,
                                                        hw_iaa_terminator_t::end_of_block);

    } else { // Static or fixed mode used
        if (state.is_first_chunk() || state.start_new_block) {
            hw_iaa_aecs_compress *actual_aecs_ptr = hw_iaa_aecs_compress_get_aecs_ptr(state.meta_data_->aecs_, actual_aecs, state.meta_data_->aecs_size);
            if (!actual_aecs_ptr) {
                result.status_code_ = status_list::internal_error;
                return result;
            }


            // If we want to write a new deflate block and it's a continuable compression task, then insert EOB
            if (!state.is_first_chunk()) {
                hw_iaa_aecs_compress_accumulator_insert_eob(actual_aecs_ptr,
                                                            state.meta_data_->eob_code);
            }

            if (state.huffman_table_) { // Static mode used
                result.status_code_ = hw_iaa_aecs_compress_write_deflate_dynamic_header(
                        actual_aecs_ptr,
                        get_deflate_header_ptr(state.huffman_table_),
                        get_deflate_header_bits_size(state.huffman_table_),
                        state.is_last_chunk());
            } else { // Fixed mode used
                result.status_code_ = hw_iaa_aecs_compress_write_deflate_fixed_header(
                        actual_aecs_ptr,
                        state.is_last_chunk());
            }
        }

        if (result.status_code_) {
            result.status_code_ = QPL_STS_LIBRARY_INTERNAL_ERR;
            return result;
        }

        if (state.is_last_chunk()) {
            /* Append EOB after final token if single job is used or if we start a new block (starting with the new Huffman Table),
               or append EOB and bFinal Stored block in case of the "middle" of multiple jobs to produce valid stream */
            hw_iaa_descriptor_compress_set_termination_rule(state.compress_descriptor_,
                                                            !(state.start_new_block || state.is_first_chunk())
                                                            ? hw_iaa_terminator_t::final_end_of_block
                                                            : hw_iaa_terminator_t::end_of_block);
        }
    }

    auto access_policy = static_cast<hw_iaa_aecs_access_policy>(util::aecs_compress_access_lookup_table[state.processing_step] |
                                                                actual_aecs);

    hw_iaa_descriptor_set_input_buffer(state.compress_descriptor_, source_begin_ptr, source_size);

    if (is_hw_1_pass_header_gen) {
        hw_iaa_descriptor_set_1_pass_header_gen(state.compress_descriptor_,
                                                state.meta_data_->aecs_,
                                                actual_aecs,
                                                state.is_last_chunk(),
                                                state.is_first_chunk());
    } else {
        hw_iaa_descriptor_compress_set_aecs(state.compress_descriptor_,
                                            state.meta_data_->aecs_,
                                            access_policy,
                                            !qpl::ml::util::are_iaa_gen_2_min_capabilities_present());
    }

    // Setup dictionary in compression descriptor
    if (dictionary) {
        const uint32_t dict_size_in_aecs         = get_dictionary_size_in_aecs(*dictionary);
        const uint8_t *const dictionary_data_ptr = get_dictionary_data(*dictionary);
        uint8_t load_dictionary_val              = get_load_dictionary_flag(*dictionary);

        hw_iaa_descriptor_compress_setup_dictionary(state.compress_descriptor_,
                                                    dict_size_in_aecs,
                                                    dictionary_data_ptr,
                                                    dictionary->aecs_raw_dictionary_offset,
                                                    state.meta_data_->aecs_,
                                                    state.meta_data_->aecs_index,
                                                    state.meta_data_->aecs_size,
                                                    load_dictionary_val);
    }

    result = util::process_descriptor<compression_operation_result_t,
             util::execution_mode_t::sync>(state.compress_descriptor_, state.completion_record_);

    if (result.status_code_ == status_list::destination_is_short_error) {
        // There can't be multiple stored blocks while indexing
        if (source_size >= 64_kb && state.meta_data_->mini_block_size_) {
            result.status_code_ = status_list::index_generation_error;
        } else {
            result = write_stored_block(state);
        }
    }

    if (state.verify_descriptor_ && !result.status_code_) {
        auto verify_actual_aecs = state.meta_data_->verify_aecs_index; // AECS used to read for verify
        result.indexes_written_ = state.prev_written_indexes;

        auto verify_access_policy = static_cast<hw_iaa_aecs_access_policy>(util::aecs_verify_access_lookup_table[state.processing_step] |
                                                                           verify_actual_aecs);

        if (state.is_first_chunk()) {
            auto initial_bit_offset = static_cast<uint32_t> (state.meta_data_->prologue_size_ * byte_bits_size);

            if (state.meta_data_->mini_block_size_) {
                // Write initial index if indexing is enabled
                hw_iaa_descriptor_compress_verification_write_initial_index(state.verify_descriptor_,
                                                                            state.aecs_verify_,
                                                                            0U,
                                                                            initial_bit_offset);
                result.indexes_written_ += 1;
            } else {
                // Just set initial decompression step otherwise
                hw_iaa_aecs_decompress_set_decompression_state(&state.aecs_verify_->inflate_options,
                                                               hw_iaa_aecs_decompress_state::hw_aecs_at_start_block_header);
            }


            if (initial_bit_offset) {
                verify_access_policy = static_cast<hw_iaa_aecs_access_policy>(verify_access_policy | hw_aecs_access_read);
            }
        }

        hw_iaa_descriptor_set_input_buffer(state.verify_descriptor_, output_begin_ptr, result.output_bytes_);

        if (verify_access_policy) {
            hw_iaa_descriptor_inflate_set_aecs(state.verify_descriptor_,
                                               state.aecs_verify_,
                                               HW_AECS_FILTER_AND_DECOMPRESS,
                                               verify_access_policy);
        }

        if (state.is_last_chunk()) {
            hw_iaa_descriptor_inflate_set_flush(state.verify_descriptor_);
        }

        auto verify_result = util::process_descriptor<verification_pass_result_t,
                                                      util::execution_mode_t::sync>(state.verify_descriptor_,
                                                                                    state.completion_record_);

        if (verify_result.status_code_ ||
            (state.is_last_chunk() && verify_result.checksums_.crc32_ != result.checksums_.crc32_)) {
            result.status_code_ = qpl::ml::status_list::verify_error;

            return result;
        }

        result.indexes_written_ += verify_result.indexes_written_;

        // Invert AECS toggle for verify
        state.meta_data_->verify_aecs_index ^= 1U;
    }

    if (result.status_code_ == status_list::ok) {
        // Invert AECS toggle for compression
        state.meta_data_->aecs_index ^= 1U;

        result.completed_bytes_ = source_size;
    }

    return result;
}

template<>
auto deflate<execution_path_t::software, deflate_mode_t::deflate_no_headers>(deflate_state<execution_path_t::software> &state,
                                                                             uint8_t *begin,
                                                                             const uint32_t size) noexcept -> compression_operation_result_t {
    state.compression_mode_ = canned_mode;
    auto output_begin_ptr   = state.next_out();

    auto result = deflate_pass(state, begin, size);

    if (state.is_verification_enabled_ && !result.status_code_) {
        auto builder = (state.is_first_chunk()) ?
                       compression::verification_state_builder<execution_path_t::software>::create(state.allocator_) :
                       compression::verification_state_builder<execution_path_t::software>::restore(state.allocator_);

        auto verify_state = builder.build();

        verify_state.input(output_begin_ptr, state.next_out())
                    .required_crc(state.checksum_.crc32);

        if (state.is_first_chunk() && state.compression_table_) {
            verify_state.decompress_table(get_deflate_header_ptr(state.compression_table_),
                                          get_deflate_header_bits_size(state.compression_table_));
        }

        auto verification_result = perform_verification<execution_path_t::software,
                                                        verification_mode_t::verify_deflate_no_headers>(verify_state);

        if (verification_result.status == parser_status_t::error) {
            result.status_code_ = qpl::ml::status_list::verify_error;

            return result;
        }
    }

    result.completed_bytes_  = state.isal_stream_ptr_->total_in;
    result.output_bytes_     = state.isal_stream_ptr_->total_out;
    result.indexes_written_  = state.index_table_.get_current_index();
    result.checksums_.crc32_ = state.checksum_.crc32;

    if (state.isal_stream_ptr_->internal_state.count) {
        result.status_code_ = qpl::ml::status_list::more_output_needed;
    }

    return result;
}

template<>
auto deflate<execution_path_t::software, deflate_mode_t::deflate_default>(deflate_state<execution_path_t::software> &state,
                                                                          uint8_t *begin,
                                                                          const uint32_t size) noexcept -> compression_operation_result_t {
    auto output_begin_ptr = state.next_out();

    compression_operation_result_t result = deflate_pass(state, begin, size);

    if (!(state.is_first_chunk() && state.is_last_chunk())) {
        state.save_bit_buffer();
    }

    if (state.is_verification_enabled_ && !result.status_code_) {
        auto builder = (state.is_first_chunk()) ?
                compression::verification_state_builder<execution_path_t::software>::create(state.allocator_) :
                compression::verification_state_builder<execution_path_t::software>::restore(state.allocator_);

        auto verify_state = builder.build();

        verify_state.input(output_begin_ptr, state.next_out())
                    .required_crc(state.checksum_.crc32);

        auto verification_result = perform_verification<execution_path_t::software,
                                                        verification_mode_t::verify_deflate_default>(verify_state);

        if (verification_result.status == parser_status_t::error) {
            result.status_code_ = qpl::ml::status_list::verify_error;

            return result;
        }
    }

    result.completed_bytes_  = state.isal_stream_ptr_->total_in;
    result.output_bytes_     = state.isal_stream_ptr_->total_out;
    result.indexes_written_  = state.index_table_.get_current_index();
    result.checksums_.crc32_ = state.checksum_.crc32;

    if (state.isal_stream_ptr_->internal_state.count) {
        result.status_code_ = qpl::ml::status_list::more_output_needed;
    }

    return result;
}

} // namespace qpl::ml::compression
